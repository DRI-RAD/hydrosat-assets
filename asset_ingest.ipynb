{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbc425e-0064-4487-accd-aa47ecdc087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "# import logging\n",
    "import os\n",
    "import pprint\n",
    "import base64\n",
    "\n",
    "\n",
    "import ee\n",
    "from google.cloud import storage\n",
    "from pyproj import Transformer\n",
    "import pystac\n",
    "from pystac_client import Client\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "import requests\n",
    "import shapely\n",
    "import shapely.ops\n",
    "\n",
    "# import IPython.display\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "\n",
    "endpoint = 'https://fusion-stac.hydrosat.com/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92327507-ef6f-45b3-b83e-bdae3f8467fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize(project='dri-hydrosat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c0cc1-8d05-45ed-88f9-b568ffc4ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('creds.json') as f:\n",
    "    creds = json.loads(f.read())\n",
    "\n",
    "userpass = f\"{creds['username']}:{creds['password']}\"\n",
    "b64 = base64.b64encode(userpass.encode()).decode()\n",
    "headers = {'Authorization':'Basic ' + b64}\n",
    "\n",
    "cat_url = 'https://stac.hydrosat.com'\n",
    "catalog = Client.open(cat_url, headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5eed7e-eadb-48bc-93f0-8b927e13b778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caballo Reservoir, Blue Mesa Reservoir, Morrow Reservoir, Crystal Reservoir, \n",
    "# Elephant Butte Reservoir, Flaming Gorge Reservoir, Navajo Reservoir, \n",
    "# Great Salt Lake, Lake Mead, Lake Powell, Lake Mohave, Lake Tahoe, Pyramid Lake, Salton Sea\n",
    "lake_geometries = {\n",
    "    'tahoe': shapely.box(*[-120.21, 38.87, -119.88, 39.28]),\n",
    "    'mead':  shapely.union(\n",
    "        shapely.box(*[-114.94, 35.98, -114.06, 36.20]), \n",
    "        shapely.box(*[-114.48, 35.98, -114.29, 36.48]), \n",
    "        grid_size=0.01),\n",
    "    # 'mead': shapely.box(*[-114.94, 35.96, -113.88, 36.48]),\n",
    "    'mohave': shapely.box(*[-114.76, 35.15, -114.52, 36.00]),\n",
    "    'powell': shapely.union(\n",
    "        shapely.box(*[-111.62, 36.84, -110.98, 37.24]),\n",
    "        shapely.box(*[-111.06, 37.02, -110.36, 37.92]),\n",
    "        grid_size=0.01),\n",
    "    # 'powell': [-111.60, 36.84, -110.26, 37.92],\n",
    "    # # 2024 \n",
    "    'bluemesa': shapely.box(*[-107.64, 38.37, -107.00, 38.55]),\n",
    "    'caballo':  shapely.box(*[-107.33, 32.87, -107.24, 33.14]),\n",
    "    'elephant': shapely.box(*[-107.24, 33.12, -107.04, 33.58]),\n",
    "    'flaming':  shapely.box(*[-109.72, 40.87, -109.36, 41.50]),\n",
    "    'great':    shapely.box(*[-113.18, 40.63, -111.88, 41.80]),\n",
    "    'havasu':   shapely.box(*[-114.46, 34.24, -114.05, 34.66]),\n",
    "    'navajo':   shapely.box(*[-107.64, 36.78, -107.30, 37.08]),\n",
    "    'pyramid':  shapely.box(*[-119.74, 39.82, -119.38, 40.23]),\n",
    "    'salton':   shapely.box(*[-116.13, 33.06, -115.55, 33.56]),\n",
    "}\n",
    "# pprint.pprint(lake_geometries)\n",
    "\n",
    "# lake_tiles = {\n",
    "#     'tahoe': ['10SGJ'],\n",
    "#     'pyramid': ['10TGK'],\n",
    "#     'mead': ['11SPV', '11SQV', '11SPA', '11SQA'],\n",
    "#     'mohave': ['11SQV'],\n",
    "#     'havasu': ['11SQU'],\n",
    "#     'powell': ['12SVF', '12SVG', '12SWG', '12SWH'],\n",
    "#     'salton': ['11SNS', '11SPT', '11SPS', '11SPT'],\n",
    "#     'great': ['12TUL', '12TUM', '12TVL'],\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626d6c27-b16f-451e-abfa-c7a5030e0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = '/Users/Charles.Morton@dri.edu/Projects/hydrosat-assets/geotiffs/'\n",
    "# workspace = os.getcwd()\n",
    "\n",
    "project_id = 'dri-hydrosat'\n",
    "bucket_name = 'hydrosat'\n",
    "\n",
    "# Output band names\n",
    "lst_band_name = 'lst'\n",
    "qa_band_name = 'qa'\n",
    "\n",
    "\n",
    "def asset_ingest(collection, extent, start_date, end_date, overwrite_flag=False):\n",
    "    print(f'Collection: {collection}')\n",
    "    print(f'Extent:     {extent}')\n",
    "    print(f'Start Date: {start_date}')\n",
    "    print(f'End Date:   {end_date}\\n')\n",
    "    \n",
    "    search = catalog.search(\n",
    "        bbox = extent,\n",
    "        #intersects = aoi,\n",
    "        datetime = [start_date, end_date],\n",
    "        collections = [collection],\n",
    "        max_items = 3000,\n",
    "    )\n",
    "    #items = search.get_all_items()\n",
    "    items = search.item_collection()\n",
    "    itemjson = items.to_dict()\n",
    "    print(f'Number of catalog items: {len(items)}\\n')\n",
    "    \n",
    "    \n",
    "    # Get the list of local file names (not paths)\n",
    "    file_list = [\n",
    "        item \n",
    "        for root, dirs, files in os.walk(workspace, topdown=False)\n",
    "        for item in files \n",
    "        if item.endswith('.tif')\n",
    "    ]\n",
    "\n",
    "    # Connect to the bucket\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "\n",
    "    \n",
    "    # Check if the asset collection needs to be built\n",
    "    asset_coll_id = f'projects/{project_id}/assets/{collection}'\n",
    "    if not ee.data.getInfo(asset_coll_id):\n",
    "        print(f'\\nCollection does not exist and will be built\\n  {asset_coll_id}')\n",
    "        input('Press ENTER to continue')\n",
    "        ee.data.createAsset({'type': 'IMAGE_COLLECTION'}, asset_coll_id)\n",
    "    \n",
    "    # Get the list of assets IDs instead of calling ee.data.getInfo in the loop\n",
    "    asset_id_list = {\n",
    "        f'{asset_coll_id}/{image_id}' \n",
    "        for image_id in ee.ImageCollection(asset_coll_id).aggregate_array('system:index').getInfo()\n",
    "    }\n",
    "    # pprint.pprint(asset_id_list)\n",
    "\n",
    "    \n",
    "    # Process each item\n",
    "    for item in itemjson[\"features\"]:\n",
    "        # pprint.pprint(item['assets'])\n",
    "        mgrs_tile = item[\"properties\"][\"mgrs_tile\"]\n",
    "        utm_zone = mgrs_tile[:2]\n",
    "        \n",
    "        lst_image_url = item[\"assets\"][\"lst\"][\"href\"]\n",
    "        qa_image_url = item[\"assets\"][\"combined_qa\"][\"href\"]\n",
    "    \n",
    "        collection = lst_image_url.split('?', 1)[0].split('/')[-4]\n",
    "        year = lst_image_url.split('?', 1)[0].split('/')[-3]\n",
    "        #temp = lst_image_url.split('?', 1)[0].split('/')[-2]\n",
    "        \n",
    "        lst_file_name = lst_image_url.split('?', 1)[0].split('/')[-1]\n",
    "        qa_file_name = qa_image_url.split('?', 1)[0].split('/')[-1]\n",
    "        # print(f'{lst_file_name}')\n",
    "        # print(f'{qa_file_name}')\n",
    "    \n",
    "        year_folder = os.path.join(workspace, collection, year)\n",
    "        lst_local_path = os.path.join(year_folder, lst_file_name)\n",
    "        qa_local_path = os.path.join(year_folder, qa_file_name)\n",
    "        lst_bucket_path = f'gs://{bucket_name}/{collection}/{year}/{lst_file_name}'\n",
    "        qa_bucket_path = f'gs://{bucket_name}/{collection}/{year}/{qa_file_name}'\n",
    "    \n",
    "        image_dt = datetime.fromisoformat(item['properties']['datetime'])\n",
    "        if 'starfm' in collection:\n",
    "            image_dt = image_dt.replace(hour=18, minute=0, second=1)\n",
    "            item['properties']['datetime'] = image_dt.isoformat()\n",
    "            # print(image_dt)\n",
    "        image_id = f'{mgrs_tile}_{image_dt.strftime(\"%Y%m%d\")}'\n",
    "        asset_id = f'{asset_coll_id}/{image_id}'\n",
    "        print(asset_id)\n",
    "\n",
    "        # if ee.data.getInfo(asset_id):\n",
    "        if asset_id in asset_id_list:\n",
    "            if overwrite_flag:\n",
    "                print(f'  Asset already exists, removing')\n",
    "                try:\n",
    "                    ee.data.deleteAsset(asset_id)\n",
    "                except Exception as e:\n",
    "                    logging.exception(f'unhandled exception: {e}')\n",
    "                    continue\n",
    "            else:\n",
    "                print(f'  Asset already exists and overwrite is False')\n",
    "                continue\n",
    "\n",
    "        # Get the properties dictionary from the item\n",
    "        # pprint.pprint(item['properties'])\n",
    "        properties = item['properties']\n",
    "        properties['file_name'] = lst_file_name\n",
    "        properties['date_ingested'] = f'{datetime.today().strftime(\"%Y-%m-%d\")}'\n",
    "    \n",
    "        # Converting properties to string\n",
    "        # This is needed especially for the nested dictionary properties\n",
    "        # TODO: Check if JSON dump would be better for this\n",
    "        str_properties = [\n",
    "            'processing:time_of_day_range', 'processing:nrt', 'processing:overwrite_outputs',\n",
    "            'processing:public', 'processing:sr_only', 'processing:test_mode',\n",
    "            'processing:qa_screen_opts', 'processing:starfm_opts', \n",
    "            'processing:pydms_common_opts', 'processing:pydms_dt_opts', \n",
    "            'processing:starfm_opts', 'hydrosat:fusion_inputs',\n",
    "        ]\n",
    "        for p in str_properties:\n",
    "            if p in properties.keys():\n",
    "                properties[p] = str(properties[p])\n",
    "        \n",
    "        # TODO: Check for a cleaner way to rename properties in a dictionary\n",
    "        del_properties = [\n",
    "            'processing:software', 'processing:pydms_nn_opts', 'processing:pydms_sknn_opts', 'processing:lineage', \n",
    "        ]\n",
    "        new_properties = {}\n",
    "        for k, v in properties.items():\n",
    "            if ((\":\" in k) or ('-' in k)) and (k not in del_properties):\n",
    "                new_properties[k.replace(':', '_').replace('-', '_')] = v\n",
    "                del_properties.append(k)\n",
    "        for k in del_properties:\n",
    "            del properties[k] \n",
    "        properties.update(new_properties)\n",
    "        #pprint.pprint(properties)\n",
    "        #input('ENTER')\n",
    "    \n",
    "    \n",
    "        # Download the image locally\n",
    "        if overwrite_flag or (lst_file_name not in file_list):\n",
    "            print('  Downloading LST image from API')\n",
    "            if not os.path.isdir(year_folder):\n",
    "                os.makedirs(year_folder)\n",
    "                \n",
    "            with requests.get(lst_image_url, stream=True) as result:\n",
    "                result.raise_for_status()\n",
    "                with open(lst_local_path, 'wb') as f:\n",
    "                    for chunk in result.iter_content(chunk_size=10000000):\n",
    "                        f.write(chunk)\n",
    "                        \n",
    "        if overwrite_flag or (qa_file_name not in file_list):\n",
    "            print('  Downloading QA image from API')\n",
    "            with requests.get(qa_image_url, stream=True) as result:\n",
    "                result.raise_for_status()\n",
    "                with open(qa_local_path, 'wb') as f:\n",
    "                    for chunk in result.iter_content(chunk_size=10000000):\n",
    "                        f.write(chunk)\n",
    "    \n",
    "        # Mask/clip images to reservoir bounding extents\n",
    "        # Project all reservoir extents to the tile projection\n",
    "        # TODO: Add some basic filtering to ignore geometries outside tile\n",
    "        transformer = Transformer.from_crs(\"EPSG:4326\", f\"EPSG:326{utm_zone}\", always_xy=True)\n",
    "        projected_lake_masks = [\n",
    "            shapely.ops.transform(transformer.transform, lake_geom)\n",
    "            for lake_nane, lake_geom in lake_geometries.items()\n",
    "        ]\n",
    "        # pprint.pprint(projected_lake_masks)\n",
    "\n",
    "        # Mask the LST and QA images\n",
    "        # The changes to the metadata are only needed if crop=True\n",
    "        #   but leaving for now\n",
    "        with rasterio.open(lst_local_path) as src:\n",
    "            out_image, out_transform = rasterio.mask.mask(src, projected_lake_masks, crop=False)\n",
    "            out_meta = src.meta\n",
    "            out_meta.update({\n",
    "                \"driver\": \"COG\",\n",
    "                # \"height\": out_image.shape[1],\n",
    "                # \"width\": out_image.shape[2],\n",
    "                # \"transform\": out_transform,\n",
    "            })\n",
    "        with rasterio.open(lst_local_path, \"w\", **out_meta) as dest:\n",
    "            dest.write(out_image)\n",
    "\n",
    "        with rasterio.open(qa_local_path) as src:\n",
    "            out_image, out_transform = rasterio.mask.mask(src, projected_lake_masks, crop=False)\n",
    "            out_meta = src.meta\n",
    "            out_meta.update({\"driver\": \"COG\"})\n",
    "        with rasterio.open(qa_local_path, \"w\", **out_meta) as dest:\n",
    "            dest.write(out_image)\n",
    "        \n",
    "        \n",
    "        # Upload the image to the bucket\n",
    "        # print('  Uploading to bucket')\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        lst_blob = bucket.blob(lst_bucket_path.replace(f'gs://{bucket_name}/', ''))\n",
    "        lst_blob.upload_from_filename(lst_local_path, timeout=120)\n",
    "        qa_blob = bucket.blob(qa_bucket_path.replace(f'gs://{bucket_name}/', ''))\n",
    "        qa_blob.upload_from_filename(qa_local_path, timeout=120)\n",
    "    \n",
    "        \n",
    "        # Ingest into Earth Engine\n",
    "        # print('  Ingesting into Earth Engine')\n",
    "        params = {\n",
    "            'name': asset_id,\n",
    "            'bands': [\n",
    "                {'id': lst_band_name, 'tilesetId': 'lst_image', 'tilesetBandIndex': 0},\n",
    "                {'id': qa_band_name, 'tilesetId': 'qa_image', 'tilesetBandIndex': 0},\n",
    "            ],\n",
    "            'tilesets': [\n",
    "                {'id': 'lst_image', 'sources': [{'uris': [lst_bucket_path]}]},\n",
    "                {'id': 'qa_image', 'sources': [{'uris': [qa_bucket_path]}]},\n",
    "            ],\n",
    "            'properties': properties,\n",
    "            'startTime': image_dt.isoformat(),\n",
    "            # 'startTime': image_dt.isoformat() + '.000000000Z',\n",
    "            # 'pyramiding_policy': 'MEAN',\n",
    "            # 'missingData': {'values': [nodata_value]},\n",
    "        }\n",
    "        task_id = ee.data.newTaskId()[0]\n",
    "        ee.data.startIngestion(task_id, params, allow_overwrite=True)\n",
    "\n",
    "        # Cleanup\n",
    "        try:\n",
    "            os.remove(lst_local_path)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            os.remove(qa_local_path)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print('Done')\n",
    "\n",
    "\n",
    "# asset_ingest(\n",
    "#     collection=\"pydms_sharpened_landsat\", \n",
    "#     extent=lake_geometries['powell'].bounds, \n",
    "#     start_date=\"2024-07-01T00:00:00Z\", \n",
    "#     end_date=\"2024-08-01T00:00:00Z\", \n",
    "#     overwrite_flag=False,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e023bb-d6c9-4ab2-800e-121518d8a898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# July 2024 @ \n",
    "# Caballo Reservoir, Blue Mesa Reservoir, Morrow Reservoir, Crystal Reservoir, Elephant Butte Reservoir, Flaming Gorge Reservoir, Navajo Reservoir, Great Salt Lake, Lake Mead, Lake Powell, Lake Mohave, Lake Tahoe, Pyramid Lake, & Salton Sea\n",
    "# April 2017 – April 2020 @ Lake Mead & Lake Mohave validation sites\n",
    "# June 2021 – June 2024 @ Lake Powell validation sites\n",
    "# June 2021 – June 2024 @ Lake Tahoe validation sites\n",
    "\n",
    "\n",
    "# First process the overpass LST July 2024 images for all reservoirs\n",
    "for lake_name, lake_geometry in lake_geometries.items():\n",
    "    print(f'\\n{lake_name}')\n",
    "    asset_ingest(\n",
    "        collection=\"pydms_sharpened_landsat\", \n",
    "        extent=lake_geometry.bounds, \n",
    "        start_date=\"2024-07-01T00:00:00Z\", \n",
    "        end_date=\"2024-08-01T00:00:00Z\", \n",
    "        overwrite_flag=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# Then process the interpolated daily LST July 2024 images for all reservoirs\n",
    "for lake_name, lake_geometry in lake_geometries.items():\n",
    "    print(f'\\n{lake_name}')\n",
    "    asset_ingest(\n",
    "        collection=\"starfm_predictions_modis_landsat\", \n",
    "        extent=lake_geometry.bounds, \n",
    "        start_date=\"2024-07-01T00:00:00Z\", \n",
    "        end_date=\"2024-08-01T00:00:00Z\", \n",
    "        overwrite_flag=False,\n",
    "    )\n",
    "\n",
    "\n",
    "# Then process the overpass LST for the full timeseries for the main reservoirs\n",
    "for year in [2024, 2023, 2022, 2021]:\n",
    "    print('\\nTahoe')\n",
    "    asset_ingest(\n",
    "        collection=\"pydms_sharpened_landsat\", \n",
    "        extent=lake_geometries['tahoe'].bounds, \n",
    "        start_date=f\"{year}-01-01T00:00:00Z\", \n",
    "        end_date=f\"{year+1}-01-01T00:00:00Z\", \n",
    "        overwrite_flag=False,\n",
    "    )\n",
    "\n",
    "for year in [2024, 2023, 2022, 2021, 2020, 2019]:\n",
    "    print('\\nPowell')\n",
    "    asset_ingest(\n",
    "        collection=\"pydms_sharpened_landsat\", \n",
    "        extent=lake_geometries['powell'].bounds, \n",
    "        start_date=f\"{year}-01-01T00:00:00Z\", \n",
    "        end_date=f\"{year+1}-01-01T00:00:00Z\", \n",
    "        overwrite_flag=False,\n",
    "    )\n",
    "for year in [2020, 2019, 2018, 2017]:\n",
    "    print('\\nMead & Mohave')\n",
    "    asset_ingest(\n",
    "        collection=\"pydms_sharpened_landsat\", \n",
    "        extent=lake_geometries['mead'].bounds, \n",
    "        start_date=f\"{year}-01-01T00:00:00Z\", \n",
    "        end_date=f\"{year+1}-01-01T00:00:00Z\", \n",
    "        overwrite_flag=False,\n",
    "    )\n",
    "\n",
    "\n",
    "# Then process the interpolated daily LST for the full timeseries for the main reservoirs\n",
    "for year in [2024, 2023, 2022, 2021]:\n",
    "    print('\\nTahoe')\n",
    "    asset_ingest(\n",
    "        collection=\"starfm_predictions_modis_landsat\", \n",
    "        extent=lake_geometries['tahoe'].bounds, \n",
    "        start_date=f\"{year}-01-01T00:00:00Z\", \n",
    "        end_date=f\"{year+1}-01-01T00:00:00Z\", \n",
    "        overwrite_flag=False,\n",
    "    )\n",
    "\n",
    "for year in [2024, 2023, 2022, 2021, 2020, 2019]:\n",
    "    print('\\nPowell')\n",
    "    asset_ingest(\n",
    "        collection=\"starfm_predictions_modis_landsat\", \n",
    "        extent=lake_geometries['powell'].bounds, \n",
    "        start_date=f\"{year}-01-01T00:00:00Z\", \n",
    "        end_date=f\"{year+1}-01-01T00:00:00Z\", \n",
    "        overwrite_flag=False,\n",
    "    )\n",
    "for year in [2020, 2019, 2018, 2017]:\n",
    "    print('\\nMead & Mohave')\n",
    "    asset_ingest(\n",
    "        collection=\"starfm_predictions_modis_landsat\", \n",
    "        extent=lake_geometries['mead'].bounds, \n",
    "        start_date=f\"{year}-01-01T00:00:00Z\", \n",
    "        end_date=f\"{year+1}-01-01T00:00:00Z\", \n",
    "        overwrite_flag=False,\n",
    "    )\n",
    "\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6cc04c-f5fb-4611-bd88-751f6423a932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9579c3-c836-4732-8b3f-4938b7af791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Then process the overpass LST for the full timeseries for the main reservoirs\n",
    "# for year in [2021, 2020, 2019]:\n",
    "#     print('\\nPowell')\n",
    "#     asset_ingest(\n",
    "#         collection=\"pydms_sharpened_landsat\", \n",
    "#         extent=lake_geometries['powell'].bounds, \n",
    "#         start_date=f\"{year}-01-01T00:00:00Z\", \n",
    "#         end_date=f\"{year+1}-01-01T00:00:00Z\", \n",
    "#         overwrite_flag=False,\n",
    "#     )\n",
    "\n",
    "# print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3299b75b-1245-484f-b198-e2dffae7de0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Then process the interpolated daily LST for the full timeseries for the main reservoirs\n",
    "# for year in [2021, 2020, 2019]:\n",
    "#     print('\\nPowell')\n",
    "#     asset_ingest(\n",
    "#         collection=\"starfm_predictions_modis_landsat\", \n",
    "#         extent=lake_geometries['powell'].bounds, \n",
    "#         start_date=f\"{year}-01-01T00:00:00Z\", \n",
    "#         end_date=f\"{year+1}-01-01T00:00:00Z\", \n",
    "#         overwrite_flag=False,\n",
    "#     )\n",
    "\n",
    "# print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fdbd75-fca0-4fda-a7b5-fd994339b743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
