{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbbc425e-0064-4487-accd-aa47ecdc087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "import base64\n",
    "\n",
    "\n",
    "import ee\n",
    "from google.cloud import storage\n",
    "from pyproj import Transformer\n",
    "import pystac\n",
    "from pystac_client import Client\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "import requests\n",
    "import shapely\n",
    "import shapely.ops\n",
    "\n",
    "# import IPython.display\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "endpoint = 'https://fusion-stac.hydrosat.com/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4760966b-537a-4f06-8696-081886a16c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to your Google Earth Engine Project ID\n",
    "project_id = 'dri-hydrosat'\n",
    "\n",
    "# TODO: Set to a Google Storage Bucket where you have write access\n",
    "bucket_name = 'hydrosat'\n",
    "\n",
    "workspace = os.getcwd()\n",
    "tif_ws = os.path.join(workspace, 'geotiffs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92327507-ef6f-45b3-b83e-bdae3f8467fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize(project=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f6c0cc1-8d05-45ed-88f9-b568ffc4ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('creds.json') as f:\n",
    "    creds = json.loads(f.read())\n",
    "\n",
    "userpass = f\"{creds['username']}:{creds['password']}\"\n",
    "b64 = base64.b64encode(userpass.encode()).decode()\n",
    "headers = {'Authorization':'Basic ' + b64}\n",
    "\n",
    "cat_url = 'https://stac.hydrosat.com'\n",
    "catalog = Client.open(cat_url, headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af5eed7e-eadb-48bc-93f0-8b927e13b778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded bounding boxes for the target reservoirs and lakes\n",
    "lake_geometries = {\n",
    "    # Lake Tahoe\n",
    "    'tahoe': shapely.box(*[-120.21, 38.87, -119.88, 39.28]),\n",
    "    # Lake Mead\n",
    "    'mead':  shapely.union(\n",
    "        shapely.box(*[-114.94, 35.98, -114.06, 36.20]), \n",
    "        shapely.box(*[-114.48, 35.98, -114.29, 36.48]), \n",
    "        grid_size=0.01),\n",
    "    # Lake Mohave\n",
    "    'mohave': shapely.box(*[-114.76, 35.15, -114.52, 36.00]),\n",
    "    # Lake Powell\n",
    "    'powell': shapely.union(\n",
    "        shapely.box(*[-111.62, 36.84, -110.98, 37.24]),\n",
    "        shapely.box(*[-111.06, 37.02, -110.36, 37.92]),\n",
    "        grid_size=0.01),\n",
    "    # Blue Mesa Reservoir\n",
    "    'bluemesa': shapely.box(*[-107.64, 38.37, -107.00, 38.55]),\n",
    "    # Caballo Reservoir\n",
    "    'caballo':  shapely.box(*[-107.33, 32.87, -107.24, 33.14]),\n",
    "    # Elephant Butte Reservoir\n",
    "    'elephant': shapely.box(*[-107.24, 33.12, -107.04, 33.58]),\n",
    "    # Flaming Gorge Reservoir\n",
    "    'flaming':  shapely.box(*[-109.72, 40.87, -109.36, 41.50]),\n",
    "    # Great Salt Lake\n",
    "    'great':    shapely.box(*[-113.18, 40.63, -111.88, 41.80]),\n",
    "    # Lake Havasu\n",
    "    'havasu':   shapely.box(*[-114.46, 34.24, -114.05, 34.66]),\n",
    "    # Navajo Reservoir\n",
    "    'navajo':   shapely.box(*[-107.64, 36.78, -107.30, 37.08]),\n",
    "    # Pyramid Lake\n",
    "    'pyramid':  shapely.box(*[-119.74, 39.82, -119.38, 40.23]),\n",
    "    # Salton Sea\n",
    "    'salton':   shapely.box(*[-116.13, 33.06, -115.55, 33.56]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "626d6c27-b16f-451e-abfa-c7a5030e0749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Assets to include in the image asset\n",
    "assets = [\n",
    "    'BLUE_SR',\n",
    "    'GREEN_SR',\n",
    "    'RED_SR',\n",
    "    'NIR_SR',\n",
    "    'LST',\n",
    "    'CLOUD_MASK',\n",
    "    'quality_assurance',\n",
    "    # 'LWIR1_BT'\n",
    "    # 'LWIR2_BT'\n",
    "    # 'REDEDGE1_SR'\n",
    "    # 'REDEDGE2_SR'\n",
    "    # 'REDEDGE3_SR'\n",
    "    # 'LWIR1_EMIS'\n",
    "    # 'LWIR2_EMIS'\n",
    "    # 'LST_UNCERTAINTY'\n",
    "    # 'THUMBNAIL_LST', \n",
    "    # 'PREVIEW_LST', \n",
    "    # 'THUMBNAIL', \n",
    "    # 'PREVIEW'\n",
    "]\n",
    "\n",
    "# Band name remapping to use for the image asset\n",
    "band_names = {\n",
    "    'BLUE_SR': 'BLUE_SR',\n",
    "    'GREEN_SR': 'GREEN_SR',\n",
    "    'RED_SR': 'RED_SR',\n",
    "    'NIR_SR': 'NIR_SR',\n",
    "    'LST': 'LST',\n",
    "    'CLOUD_MASK': 'CLOUD_MASK',\n",
    "    'quality_assurance': 'QA',\n",
    "}\n",
    "\n",
    "\n",
    "def asset_ingest(collection, extent, start_date, end_date, overwrite_flag=False):\n",
    "    \"\"\"Function for ingesting FusionHub LST and QA images into Earth Engine\"\"\"\n",
    "    \n",
    "    print(f'Collection: {collection}')\n",
    "    print(f'Extent:     {extent}')\n",
    "    print(f'Start Date: {start_date}')\n",
    "    print(f'End Date:   {end_date}\\n')\n",
    "    \n",
    "    search = catalog.search(\n",
    "        bbox = extent,\n",
    "        datetime = [start_date, end_date],\n",
    "        collections = [collection],\n",
    "        max_items = 3000,\n",
    "    )\n",
    "    #items = search.get_all_items()\n",
    "    items = search.item_collection()\n",
    "    itemjson = items.to_dict()\n",
    "    print(f'Number of catalog items: {len(items)}\\n')\n",
    "    \n",
    "    # Get the list of local file names (not paths)\n",
    "    file_list = [\n",
    "        item \n",
    "        for root, dirs, files in os.walk(tif_ws, topdown=False)\n",
    "        for item in files \n",
    "        if item.endswith('.tif')\n",
    "    ]\n",
    "\n",
    "    # Connect to the bucket\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    \n",
    "    # Check if the asset collection needs to be built\n",
    "    asset_coll_id = f'projects/{project_id}/assets/{collection}'\n",
    "    if not ee.data.getInfo(asset_coll_id):\n",
    "        print(f'\\nCollection does not exist and will be built\\n  {asset_coll_id}')\n",
    "        input('Press ENTER to continue')\n",
    "        ee.data.createAsset({'type': 'IMAGE_COLLECTION'}, asset_coll_id)\n",
    "    \n",
    "    # Get the list of assets IDs instead of calling ee.data.getInfo in the loop\n",
    "    asset_id_list = {\n",
    "        f'{asset_coll_id}/{image_id}' \n",
    "        for image_id in ee.ImageCollection(asset_coll_id).aggregate_array('system:index').getInfo()\n",
    "    }\n",
    "    # pprint.pprint(asset_id_list)\n",
    "\n",
    "    \n",
    "    # Process each item\n",
    "    for item in itemjson[\"features\"]:\n",
    "\n",
    "        image_dt = datetime.fromisoformat(item['properties']['datetime'])\n",
    "        if 'starfm' in collection:\n",
    "            image_dt = image_dt.replace(hour=18, minute=0, second=1)\n",
    "            item['properties']['datetime'] = image_dt.isoformat()\n",
    "        epsg = item['properties']['proj:epsg']\n",
    "\n",
    "        image_id = item['properties']['hydrosat:scene_id']\n",
    "        asset_id = f'{asset_coll_id}/{image_id}'\n",
    "        print(asset_id)\n",
    "\n",
    "        # if ee.data.getInfo(asset_id):\n",
    "        if asset_id in asset_id_list:\n",
    "            if overwrite_flag:\n",
    "                print(f'  Asset already exists, removing')\n",
    "                try:\n",
    "                    ee.data.deleteAsset(asset_id)\n",
    "                except Exception as e:\n",
    "                    logging.exception(f'unhandled exception: {e}')\n",
    "                    continue\n",
    "            else:\n",
    "                print(f'  Asset already exists and overwrite is False')\n",
    "                continue\n",
    "\n",
    "        year_folder = os.path.join(tif_ws, collection, str(image_dt.year))\n",
    "        if not os.path.isdir(year_folder):\n",
    "            os.makedirs(year_folder)\n",
    "        \n",
    "        # Get the properties dictionary from the item\n",
    "        properties = item['properties']\n",
    "        properties['date_ingested'] = f'{datetime.today().strftime(\"%Y-%m-%d\")}'\n",
    "\n",
    "        # Converting properties to string\n",
    "        # This is needed especially for the nested dictionary properties\n",
    "        # TODO: Check if JSON dump would be better for this\n",
    "        str_properties = [\n",
    "            'instruments',\n",
    "            'proj:centroid',\n",
    "            # 'processing:software',\n",
    "        ]\n",
    "        for p in str_properties:\n",
    "            if p in properties.keys():\n",
    "                properties[p] = str(properties[p])\n",
    "        \n",
    "        # TODO: Check for a cleaner way to rename properties in a dictionary\n",
    "        del_properties = [\n",
    "            'processing:software', \n",
    "            'processing:pydms_nn_opts', \n",
    "            'processing:pydms_sknn_opts', \n",
    "            # 'processing:lineage', \n",
    "        ]\n",
    "        new_properties = {}\n",
    "        for k, v in properties.items():\n",
    "            if ((\":\" in k) or ('-' in k)) and (k not in del_properties):\n",
    "                new_properties[k.replace(':', '_').replace('-', '_')] = v\n",
    "                del_properties.append(k)\n",
    "        for k in del_properties:\n",
    "            if k in properties:\n",
    "                del properties[k] \n",
    "        properties.update(new_properties)\n",
    "\n",
    "        # Keep a link to all of the bucket paths for building the manifest\n",
    "        bucket_paths = {}\n",
    "\n",
    "        \n",
    "        # Process each asset in the item\n",
    "        for asset_name in item[\"assets\"].keys():\n",
    "            if asset_name not in assets:\n",
    "                continue\n",
    "            # print(f'  Asset: {asset_name}')\n",
    "            \n",
    "            image_url = item[\"assets\"][asset_name][\"href\"]\n",
    "\n",
    "            file_name = image_url.split('?', 1)[0].split('/')[-1]\n",
    "            # print(f'  {file_name}')\n",
    "        \n",
    "            local_path = os.path.join(year_folder, file_name)\n",
    "            bucket_path = f'gs://{bucket_name}/{collection}/{image_dt.year}/{file_name}'\n",
    "        \n",
    "            # Download the image locally\n",
    "            if overwrite_flag or (file_name not in file_list):\n",
    "                print(f'  Downloading {asset_name} image from API')\n",
    "                if not os.path.isdir(year_folder):\n",
    "                    os.makedirs(year_folder)\n",
    "                    \n",
    "                with requests.get(image_url, stream=True) as result:\n",
    "                    result.raise_for_status()\n",
    "                    with open(local_path, 'wb') as f:\n",
    "                        for chunk in result.iter_content(chunk_size=10000000):\n",
    "                            f.write(chunk)\n",
    "\n",
    "            # Mask/clip images to reservoir bounding extents\n",
    "            # Project all reservoir extents to the tile projection\n",
    "            # TODO: Add some basic filtering to ignore geometries outside tile\n",
    "            transformer = Transformer.from_crs(\"EPSG:4326\", f\"EPSG:{epsg}\", always_xy=True)\n",
    "            projected_lake_masks = [\n",
    "                shapely.ops.transform(transformer.transform, lake_geom)\n",
    "                for lake_nane, lake_geom in lake_geometries.items()\n",
    "            ]\n",
    "            # pprint.pprint(projected_lake_masks)\n",
    "\n",
    "            # Mask the non-land portions of the images\n",
    "            # # The changes to the metadata are only needed if crop=True but leaving for now\n",
    "            with rasterio.open(local_path) as src:\n",
    "                out_image, out_transform = rasterio.mask.mask(src, projected_lake_masks, crop=False)\n",
    "                out_meta = src.meta\n",
    "                out_meta.update({\n",
    "                    \"driver\": \"COG\",\n",
    "                    # \"height\": out_image.shape[1],\n",
    "                    # \"width\": out_image.shape[2],\n",
    "                    # \"transform\": out_transform,\n",
    "                })\n",
    "            with rasterio.open(local_path, \"w\", **out_meta) as dest:\n",
    "                dest.write(out_image)\n",
    "\n",
    "            # TODO: Add support for scaling the images before ingesting\n",
    "            # TODO: Add support for masking based on the nodata value?\n",
    "            raster_bands_info = item[\"assets\"][asset_name]['raster:bands'][0]\n",
    "            for p in ['nodata', 'scale', 'spatial_resolution', 'unit']:\n",
    "                if p in raster_bands_info.keys():\n",
    "                    properties[f'{band_names[asset_name]}_{p}'] = raster_bands_info[p]\n",
    "    \n",
    "            # Upload the image to the bucket\n",
    "            # print('  Uploading to bucket')\n",
    "            bucket = storage_client.bucket(bucket_name)\n",
    "            blob = bucket.blob(bucket_path.replace(f'gs://{bucket_name}/', ''))\n",
    "            blob.upload_from_filename(local_path, timeout=120)\n",
    "            bucket_paths[asset_name] = bucket_path\n",
    "\n",
    "            # # Cleanup\n",
    "            # try:\n",
    "            #     os.remove(local_path)\n",
    "            # except:\n",
    "            #     pass\n",
    "\n",
    "            \n",
    "        # Ingest into Earth Engine\n",
    "        print('  Ingesting into Earth Engine')\n",
    "        params = {\n",
    "            'name': asset_id,\n",
    "            'bands': [\n",
    "                {'id': band_names[asset_name], 'tilesetId': asset_name, 'tilesetBandIndex': 0} \n",
    "                for asset_name in assets\n",
    "            ],\n",
    "            'tilesets': [\n",
    "                {'id': asset_name, 'sources': [{'uris': [bucket_paths[asset_name]]}]} \n",
    "                for asset_name in assets\n",
    "            ],\n",
    "            'properties': properties,\n",
    "            'startTime': image_dt.isoformat(),\n",
    "            # 'startTime': image_dt.isoformat() + '.000000000Z',\n",
    "            # 'pyramiding_policy': 'MEAN',\n",
    "            # 'missingData': {'values': [nodata_value]},\n",
    "        }\n",
    "        task_id = ee.data.newTaskId()[0]\n",
    "        ee.data.startIngestion(task_id, params, allow_overwrite=True)\n",
    "\n",
    "print('Done')\n",
    "\n",
    "\n",
    "# asset_ingest(\n",
    "#     collection=\"vz-l2\",\n",
    "#     extent=lake_geometries['powell'].bounds, \n",
    "#     start_date=\"2025-06-01T00:00:00Z\", \n",
    "#     end_date=\"2025-08-01T00:00:00Z\", \n",
    "#     overwrite_flag=False,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db5409b7-7d54-487d-a726-cdec8356e707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tahoe\n",
      "Collection: vz-l2\n",
      "Extent:     (-120.21, 38.87, -119.88, 39.28)\n",
      "Start Date: 2025-06-01T00:00:00Z\n",
      "End Date:   2025-08-01T00:00:00Z\n",
      "\n",
      "Number of catalog items: 2\n",
      "\n",
      "projects/dri-hydrosat/assets/vz-l2/VZ01_L2_20250604_190449\n",
      "  Asset already exists, removing\n",
      "  Asset: BLUE_SR\n",
      "  Downloading BLUE_SR image from API\n",
      "  Asset: GREEN_SR\n",
      "  Downloading GREEN_SR image from API\n",
      "  Asset: NIR_SR\n",
      "  Downloading NIR_SR image from API\n",
      "  Asset: RED_SR\n",
      "  Downloading RED_SR image from API\n",
      "  Asset: LST\n",
      "  Downloading LST image from API\n",
      "  Asset: quality_assurance\n",
      "  Downloading quality_assurance image from API\n",
      "  Asset: CLOUD_MASK\n",
      "  Downloading CLOUD_MASK image from API\n",
      "  Ingesting into Earth Engine\n",
      "projects/dri-hydrosat/assets/vz-l2/VZ01_L2_20250604_190439\n",
      "  Asset already exists, removing\n",
      "  Asset: BLUE_SR\n",
      "  Downloading BLUE_SR image from API\n",
      "  Asset: GREEN_SR\n",
      "  Downloading GREEN_SR image from API\n",
      "  Asset: NIR_SR\n",
      "  Downloading NIR_SR image from API\n",
      "  Asset: RED_SR\n",
      "  Downloading RED_SR image from API\n",
      "  Asset: LST\n",
      "  Downloading LST image from API\n",
      "  Asset: quality_assurance\n",
      "  Downloading quality_assurance image from API\n",
      "  Asset: CLOUD_MASK\n",
      "  Downloading CLOUD_MASK image from API\n",
      "  Ingesting into Earth Engine\n",
      "\n",
      "mead\n",
      "Collection: vz-l2\n",
      "Extent:     (-114.94, 35.98, -114.06, 36.48)\n",
      "Start Date: 2025-06-01T00:00:00Z\n",
      "End Date:   2025-08-01T00:00:00Z\n",
      "\n",
      "Number of catalog items: 0\n",
      "\n",
      "\n",
      "mohave\n",
      "Collection: vz-l2\n",
      "Extent:     (-114.76, 35.15, -114.52, 36.0)\n",
      "Start Date: 2025-06-01T00:00:00Z\n",
      "End Date:   2025-08-01T00:00:00Z\n",
      "\n",
      "Number of catalog items: 0\n",
      "\n",
      "\n",
      "powell\n",
      "Collection: vz-l2\n",
      "Extent:     (-111.62, 36.84, -110.36, 37.92)\n",
      "Start Date: 2025-06-01T00:00:00Z\n",
      "End Date:   2025-08-01T00:00:00Z\n",
      "\n",
      "Number of catalog items: 3\n",
      "\n",
      "projects/dri-hydrosat/assets/vz-l2/VZ01_L2_20250614_182420\n",
      "  Asset already exists, removing\n",
      "  Asset: BLUE_SR\n",
      "  Downloading BLUE_SR image from API\n",
      "  Asset: GREEN_SR\n",
      "  Downloading GREEN_SR image from API\n",
      "  Asset: NIR_SR\n",
      "  Downloading NIR_SR image from API\n",
      "  Asset: RED_SR\n",
      "  Downloading RED_SR image from API\n",
      "  Asset: LST\n",
      "  Downloading LST image from API\n",
      "  Asset: quality_assurance\n",
      "  Downloading quality_assurance image from API\n",
      "  Asset: CLOUD_MASK\n",
      "  Downloading CLOUD_MASK image from API\n",
      "  Ingesting into Earth Engine\n",
      "projects/dri-hydrosat/assets/vz-l2/VZ01_L2_20250614_182410\n",
      "  Asset: BLUE_SR\n",
      "  Downloading BLUE_SR image from API\n",
      "  Asset: GREEN_SR\n",
      "  Downloading GREEN_SR image from API\n",
      "  Asset: NIR_SR\n",
      "  Downloading NIR_SR image from API\n",
      "  Asset: RED_SR\n",
      "  Downloading RED_SR image from API\n",
      "  Asset: LST\n",
      "  Downloading LST image from API\n",
      "  Asset: quality_assurance\n",
      "  Downloading quality_assurance image from API\n",
      "  Asset: CLOUD_MASK\n",
      "  Downloading CLOUD_MASK image from API\n",
      "  Ingesting into Earth Engine\n",
      "projects/dri-hydrosat/assets/vz-l2/VZ01_L2_20250614_182401\n",
      "  Asset: BLUE_SR\n",
      "  Downloading BLUE_SR image from API\n",
      "  Asset: GREEN_SR\n",
      "  Downloading GREEN_SR image from API\n",
      "  Asset: NIR_SR\n",
      "  Downloading NIR_SR image from API\n",
      "  Asset: RED_SR\n",
      "  Downloading RED_SR image from API\n",
      "  Asset: LST\n",
      "  Downloading LST image from API\n",
      "  Asset: quality_assurance\n",
      "  Downloading quality_assurance image from API\n",
      "  Asset: CLOUD_MASK\n",
      "  Downloading CLOUD_MASK image from API\n",
      "  Ingesting into Earth Engine\n",
      "\n",
      "bluemesa\n",
      "Collection: vz-l2\n",
      "Extent:     (-107.64, 38.37, -107.0, 38.55)\n",
      "Start Date: 2025-06-01T00:00:00Z\n",
      "End Date:   2025-08-01T00:00:00Z\n",
      "\n",
      "Number of catalog items: 0\n",
      "\n",
      "\n",
      "caballo\n",
      "Collection: vz-l2\n",
      "Extent:     (-107.33, 32.87, -107.24, 33.14)\n",
      "Start Date: 2025-06-01T00:00:00Z\n",
      "End Date:   2025-08-01T00:00:00Z\n",
      "\n",
      "Number of catalog items: 0\n",
      "\n",
      "\n",
      "elephant\n",
      "Collection: vz-l2\n",
      "Extent:     (-107.24, 33.12, -107.04, 33.58)\n",
      "Start Date: 2025-06-01T00:00:00Z\n",
      "End Date:   2025-08-01T00:00:00Z\n",
      "\n",
      "Number of catalog items: 0\n",
      "\n",
      "\n",
      "flaming\n",
      "Collection: vz-l2\n",
      "Extent:     (-109.72, 40.87, -109.36, 41.5)\n",
      "Start Date: 2025-06-01T00:00:00Z\n",
      "End Date:   2025-08-01T00:00:00Z\n",
      "\n",
      "Number of catalog items: 2\n",
      "\n",
      "projects/dri-hydrosat/assets/vz-l2/VZ01_L2_20250614_182313\n",
      "  Asset: BLUE_SR\n",
      "  Downloading BLUE_SR image from API\n",
      "  Asset: GREEN_SR\n",
      "  Downloading GREEN_SR image from API\n",
      "  Asset: NIR_SR\n",
      "  Downloading NIR_SR image from API\n",
      "  Asset: RED_SR\n",
      "  Downloading RED_SR image from API\n",
      "  Asset: LST\n",
      "  Downloading LST image from API\n",
      "  Asset: quality_assurance\n",
      "  Downloading quality_assurance image from API\n",
      "  Asset: CLOUD_MASK\n",
      "  Downloading CLOUD_MASK image from API\n",
      "  Ingesting into Earth Engine\n",
      "projects/dri-hydrosat/assets/vz-l2/VZ01_L2_20250614_182304\n",
      "  Asset: BLUE_SR\n",
      "  Downloading BLUE_SR image from API\n",
      "  Asset: GREEN_SR\n",
      "  Downloading GREEN_SR image from API\n",
      "  Asset: NIR_SR\n",
      "  Downloading NIR_SR image from API\n",
      "  Asset: RED_SR\n",
      "  Downloading RED_SR image from API\n",
      "  Asset: LST\n",
      "  Downloading LST image from API\n",
      "  Asset: quality_assurance\n",
      "  Downloading quality_assurance image from API\n",
      "  Asset: CLOUD_MASK\n",
      "  Downloading CLOUD_MASK image from API\n",
      "  Ingesting into Earth Engine\n",
      "\n",
      "great\n",
      "Collection: vz-l2\n",
      "Extent:     (-113.18, 40.63, -111.88, 41.8)\n",
      "Start Date: 2025-06-01T00:00:00Z\n",
      "End Date:   2025-08-01T00:00:00Z\n",
      "\n",
      "Number of catalog items: 0\n",
      "\n",
      "\n",
      "havasu\n",
      "Collection: vz-l2\n",
      "Extent:     (-114.46, 34.24, -114.05, 34.66)\n",
      "Start Date: 2025-06-01T00:00:00Z\n",
      "End Date:   2025-08-01T00:00:00Z\n",
      "\n",
      "Number of catalog items: 0\n",
      "\n",
      "\n",
      "navajo\n",
      "Collection: vz-l2\n",
      "Extent:     (-107.64, 36.78, -107.3, 37.08)\n",
      "Start Date: 2025-06-01T00:00:00Z\n",
      "End Date:   2025-08-01T00:00:00Z\n",
      "\n",
      "Number of catalog items: 0\n",
      "\n",
      "\n",
      "pyramid\n",
      "Collection: vz-l2\n",
      "Extent:     (-119.74, 39.82, -119.38, 40.23)\n",
      "Start Date: 2025-06-01T00:00:00Z\n",
      "End Date:   2025-08-01T00:00:00Z\n",
      "\n",
      "Number of catalog items: 2\n",
      "\n",
      "projects/dri-hydrosat/assets/vz-l2/VZ01_L2_20250604_190430\n",
      "  Asset: BLUE_SR\n",
      "  Downloading BLUE_SR image from API\n",
      "  Asset: GREEN_SR\n",
      "  Downloading GREEN_SR image from API\n",
      "  Asset: NIR_SR\n",
      "  Downloading NIR_SR image from API\n",
      "  Asset: RED_SR\n",
      "  Downloading RED_SR image from API\n",
      "  Asset: LST\n",
      "  Downloading LST image from API\n",
      "  Asset: quality_assurance\n",
      "  Downloading quality_assurance image from API\n",
      "  Asset: CLOUD_MASK\n",
      "  Downloading CLOUD_MASK image from API\n",
      "  Ingesting into Earth Engine\n",
      "projects/dri-hydrosat/assets/vz-l2/VZ01_L2_20250604_190420\n",
      "  Asset: BLUE_SR\n",
      "  Downloading BLUE_SR image from API\n",
      "  Asset: GREEN_SR\n",
      "  Downloading GREEN_SR image from API\n",
      "  Asset: NIR_SR\n",
      "  Downloading NIR_SR image from API\n",
      "  Asset: RED_SR\n",
      "  Downloading RED_SR image from API\n",
      "  Asset: LST\n",
      "  Downloading LST image from API\n",
      "  Asset: quality_assurance\n",
      "  Downloading quality_assurance image from API\n",
      "  Asset: CLOUD_MASK\n",
      "  Downloading CLOUD_MASK image from API\n",
      "  Ingesting into Earth Engine\n",
      "\n",
      "salton\n",
      "Collection: vz-l2\n",
      "Extent:     (-116.13, 33.06, -115.55, 33.56)\n",
      "Start Date: 2025-06-01T00:00:00Z\n",
      "End Date:   2025-08-01T00:00:00Z\n",
      "\n",
      "Number of catalog items: 0\n",
      "\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Process the overpass LST July 2024 images for all reservoirs\n",
    "for lake_name, lake_geometry in lake_geometries.items():\n",
    "    print(f'\\n{lake_name}')\n",
    "    asset_ingest(\n",
    "        collection=\"vz-l2\", \n",
    "        extent=lake_geometry.bounds, \n",
    "        start_date=\"2025-06-01T00:00:00Z\", \n",
    "        end_date=\"2025-08-01T00:00:00Z\", \n",
    "        overwrite_flag=True,\n",
    "    )\n",
    "\n",
    "    # break\n",
    "\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc016e7-7e6a-493e-b0f5-a9a8b62cee50",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Output band names\n",
    "# #lst_band_name = 'lst'\n",
    "# #qa_band_name = 'qa'\n",
    "\n",
    "# # TODO: Write docstring\n",
    "# def asset_ingest(collection, extent, start_date, end_date, overwrite_flag=False):\n",
    "#     \"\"\"Function for ingesting FusionHub LST and QA images into Earth Engine\"\"\"\n",
    "    \n",
    "#     print(f'Collection: {collection}')\n",
    "#     print(f'Extent:     {extent}')\n",
    "#     print(f'Start Date: {start_date}')\n",
    "#     print(f'End Date:   {end_date}\\n')\n",
    "    \n",
    "#     search = catalog.search(\n",
    "#         bbox = extent,\n",
    "#         #intersects = aoi,\n",
    "#         datetime = [start_date, end_date],\n",
    "#         collections = [collection],\n",
    "#         max_items = 3000,\n",
    "#     )\n",
    "#     #items = search.get_all_items()\n",
    "#     items = search.item_collection()\n",
    "#     itemjson = items.to_dict()\n",
    "#     print(f'Number of catalog items: {len(items)}\\n')\n",
    "    \n",
    "    \n",
    "#     # Get the list of local file names (not paths)\n",
    "#     file_list = [\n",
    "#         item \n",
    "#         for root, dirs, files in os.walk(tif_ws, topdown=False)\n",
    "#         for item in files \n",
    "#         if item.endswith('.tif')\n",
    "#     ]\n",
    "\n",
    "#     # Connect to the bucket\n",
    "#     storage_client = storage.Client(project=project_id)\n",
    "\n",
    "    \n",
    "#     # Check if the asset collection needs to be built\n",
    "#     asset_coll_id = f'projects/{project_id}/assets/{collection}'\n",
    "#     if not ee.data.getInfo(asset_coll_id):\n",
    "#         print(f'\\nCollection does not exist and will be built\\n  {asset_coll_id}')\n",
    "#         input('Press ENTER to continue')\n",
    "#         ee.data.createAsset({'type': 'IMAGE_COLLECTION'}, asset_coll_id)\n",
    "    \n",
    "#     # Get the list of assets IDs instead of calling ee.data.getInfo in the loop\n",
    "#     asset_id_list = {\n",
    "#         f'{asset_coll_id}/{image_id}' \n",
    "#         for image_id in ee.ImageCollection(asset_coll_id).aggregate_array('system:index').getInfo()\n",
    "#     }\n",
    "#     # pprint.pprint(asset_id_list)\n",
    "\n",
    "    \n",
    "#     # Process each item\n",
    "#     for item in itemjson[\"features\"]:\n",
    "#         # pprint.pprint(item['assets'])\n",
    "#         mgrs_tile = item[\"properties\"][\"mgrs_tile\"]\n",
    "#         utm_zone = mgrs_tile[:2]\n",
    "        \n",
    "#         lst_image_url = item[\"assets\"][\"lst\"][\"href\"]\n",
    "#         qa_image_url = item[\"assets\"][\"combined_qa\"][\"href\"]\n",
    "    \n",
    "#         collection = lst_image_url.split('?', 1)[0].split('/')[-4]\n",
    "#         year = lst_image_url.split('?', 1)[0].split('/')[-3]\n",
    "#         #temp = lst_image_url.split('?', 1)[0].split('/')[-2]\n",
    "        \n",
    "#         lst_file_name = lst_image_url.split('?', 1)[0].split('/')[-1]\n",
    "#         qa_file_name = qa_image_url.split('?', 1)[0].split('/')[-1]\n",
    "#         # print(f'{lst_file_name}')\n",
    "#         # print(f'{qa_file_name}')\n",
    "    \n",
    "#         year_folder = os.path.join(tif_ws, collection, year)\n",
    "#         lst_local_path = os.path.join(year_folder, lst_file_name)\n",
    "#         qa_local_path = os.path.join(year_folder, qa_file_name)\n",
    "#         lst_bucket_path = f'gs://{bucket_name}/{collection}/{year}/{lst_file_name}'\n",
    "#         qa_bucket_path = f'gs://{bucket_name}/{collection}/{year}/{qa_file_name}'\n",
    "    \n",
    "#         image_dt = datetime.fromisoformat(item['properties']['datetime'])\n",
    "#         if 'starfm' in collection:\n",
    "#             image_dt = image_dt.replace(hour=18, minute=0, second=1)\n",
    "#             item['properties']['datetime'] = image_dt.isoformat()\n",
    "#             # print(image_dt)\n",
    "#         image_id = f'{mgrs_tile}_{image_dt.strftime(\"%Y%m%d\")}'\n",
    "#         asset_id = f'{asset_coll_id}/{image_id}'\n",
    "#         print(asset_id)\n",
    "\n",
    "#         # if ee.data.getInfo(asset_id):\n",
    "#         if asset_id in asset_id_list:\n",
    "#             if overwrite_flag:\n",
    "#                 print(f'  Asset already exists, removing')\n",
    "#                 try:\n",
    "#                     ee.data.deleteAsset(asset_id)\n",
    "#                 except Exception as e:\n",
    "#                     logging.exception(f'unhandled exception: {e}')\n",
    "#                     continue\n",
    "#             else:\n",
    "#                 print(f'  Asset already exists and overwrite is False')\n",
    "#                 continue\n",
    "\n",
    "#         # Get the properties dictionary from the item\n",
    "#         # pprint.pprint(item['properties'])\n",
    "#         properties = item['properties']\n",
    "#         properties['file_name'] = lst_file_name\n",
    "#         properties['date_ingested'] = f'{datetime.today().strftime(\"%Y-%m-%d\")}'\n",
    "    \n",
    "#         # Converting properties to string\n",
    "#         # This is needed especially for the nested dictionary properties\n",
    "#         # TODO: Check if JSON dump would be better for this\n",
    "#         str_properties = [\n",
    "#             'processing:time_of_day_range', 'processing:nrt', 'processing:overwrite_outputs',\n",
    "#             'processing:public', 'processing:sr_only', 'processing:test_mode',\n",
    "#             'processing:qa_screen_opts', 'processing:starfm_opts', \n",
    "#             'processing:pydms_common_opts', 'processing:pydms_dt_opts', \n",
    "#             'processing:starfm_opts', 'hydrosat:fusion_inputs',\n",
    "#         ]\n",
    "#         for p in str_properties:\n",
    "#             if p in properties.keys():\n",
    "#                 properties[p] = str(properties[p])\n",
    "        \n",
    "#         # TODO: Check for a cleaner way to rename properties in a dictionary\n",
    "#         del_properties = [\n",
    "#             'processing:software', 'processing:pydms_nn_opts', 'processing:pydms_sknn_opts', 'processing:lineage', \n",
    "#         ]\n",
    "#         new_properties = {}\n",
    "#         for k, v in properties.items():\n",
    "#             if ((\":\" in k) or ('-' in k)) and (k not in del_properties):\n",
    "#                 new_properties[k.replace(':', '_').replace('-', '_')] = v\n",
    "#                 del_properties.append(k)\n",
    "#         for k in del_properties:\n",
    "#             del properties[k] \n",
    "#         properties.update(new_properties)\n",
    "#         #pprint.pprint(properties)\n",
    "#         #input('ENTER')\n",
    "    \n",
    "    \n",
    "#         # Download the image locally\n",
    "#         if overwrite_flag or (lst_file_name not in file_list):\n",
    "#             print('  Downloading LST image from API')\n",
    "#             if not os.path.isdir(year_folder):\n",
    "#                 os.makedirs(year_folder)\n",
    "                \n",
    "#             with requests.get(lst_image_url, stream=True) as result:\n",
    "#                 result.raise_for_status()\n",
    "#                 with open(lst_local_path, 'wb') as f:\n",
    "#                     for chunk in result.iter_content(chunk_size=10000000):\n",
    "#                         f.write(chunk)\n",
    "                        \n",
    "#         if overwrite_flag or (qa_file_name not in file_list):\n",
    "#             print('  Downloading QA image from API')\n",
    "#             with requests.get(qa_image_url, stream=True) as result:\n",
    "#                 result.raise_for_status()\n",
    "#                 with open(qa_local_path, 'wb') as f:\n",
    "#                     for chunk in result.iter_content(chunk_size=10000000):\n",
    "#                         f.write(chunk)\n",
    "    \n",
    "#         # Mask/clip images to reservoir bounding extents\n",
    "#         # Project all reservoir extents to the tile projection\n",
    "#         # TODO: Add some basic filtering to ignore geometries outside tile\n",
    "#         transformer = Transformer.from_crs(\"EPSG:4326\", f\"EPSG:326{utm_zone}\", always_xy=True)\n",
    "#         projected_lake_masks = [\n",
    "#             shapely.ops.transform(transformer.transform, lake_geom)\n",
    "#             for lake_nane, lake_geom in lake_geometries.items()\n",
    "#         ]\n",
    "#         # pprint.pprint(projected_lake_masks)\n",
    "\n",
    "#         # Mask the LST and QA images\n",
    "#         # The changes to the metadata are only needed if crop=True\n",
    "#         #   but leaving for now\n",
    "#         with rasterio.open(lst_local_path) as src:\n",
    "#             out_image, out_transform = rasterio.mask.mask(src, projected_lake_masks, crop=False)\n",
    "#             out_meta = src.meta\n",
    "#             out_meta.update({\n",
    "#                 \"driver\": \"COG\",\n",
    "#                 # \"height\": out_image.shape[1],\n",
    "#                 # \"width\": out_image.shape[2],\n",
    "#                 # \"transform\": out_transform,\n",
    "#             })\n",
    "#         with rasterio.open(lst_local_path, \"w\", **out_meta) as dest:\n",
    "#             dest.write(out_image)\n",
    "\n",
    "#         with rasterio.open(qa_local_path) as src:\n",
    "#             out_image, out_transform = rasterio.mask.mask(src, projected_lake_masks, crop=False)\n",
    "#             out_meta = src.meta\n",
    "#             out_meta.update({\"driver\": \"COG\"})\n",
    "#         with rasterio.open(qa_local_path, \"w\", **out_meta) as dest:\n",
    "#             dest.write(out_image)\n",
    "        \n",
    "        \n",
    "#         # Upload the image to the bucket\n",
    "#         # print('  Uploading to bucket')\n",
    "#         bucket = storage_client.bucket(bucket_name)\n",
    "#         lst_blob = bucket.blob(lst_bucket_path.replace(f'gs://{bucket_name}/', ''))\n",
    "#         lst_blob.upload_from_filename(lst_local_path, timeout=120)\n",
    "#         qa_blob = bucket.blob(qa_bucket_path.replace(f'gs://{bucket_name}/', ''))\n",
    "#         qa_blob.upload_from_filename(qa_local_path, timeout=120)\n",
    "    \n",
    "        \n",
    "#         # Ingest into Earth Engine\n",
    "#         # print('  Ingesting into Earth Engine')\n",
    "#         params = {\n",
    "#             'name': asset_id,\n",
    "#             'bands': [\n",
    "#                 {'id': lst_band_name, 'tilesetId': 'lst_image', 'tilesetBandIndex': 0},\n",
    "#                 {'id': qa_band_name, 'tilesetId': 'qa_image', 'tilesetBandIndex': 0},\n",
    "#             ],\n",
    "#             'tilesets': [\n",
    "#                 {'id': 'lst_image', 'sources': [{'uris': [lst_bucket_path]}]},\n",
    "#                 {'id': 'qa_image', 'sources': [{'uris': [qa_bucket_path]}]},\n",
    "#             ],\n",
    "#             'properties': properties,\n",
    "#             'startTime': image_dt.isoformat(),\n",
    "#             # 'startTime': image_dt.isoformat() + '.000000000Z',\n",
    "#             # 'pyramiding_policy': 'MEAN',\n",
    "#             # 'missingData': {'values': [nodata_value]},\n",
    "#         }\n",
    "#         task_id = ee.data.newTaskId()[0]\n",
    "#         ee.data.startIngestion(task_id, params, allow_overwrite=True)\n",
    "\n",
    "#         # Cleanup\n",
    "#         try:\n",
    "#             os.remove(lst_local_path)\n",
    "#         except:\n",
    "#             pass\n",
    "#         try:\n",
    "#             os.remove(qa_local_path)\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "# print('Done')\n",
    "\n",
    "\n",
    "# asset_ingest(\n",
    "#     collection=\"pydms_sharpened_landsat\", \n",
    "#     extent=lake_geometries['powell'].bounds, \n",
    "#     start_date=\"2024-07-01T00:00:00Z\", \n",
    "#     end_date=\"2025-08-01T00:00:00Z\", \n",
    "#     overwrite_flag=False,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e023bb-d6c9-4ab2-800e-121518d8a898",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # July 2024 @ \n",
    "# # Caballo Reservoir, Blue Mesa Reservoir, Morrow Reservoir, Crystal Reservoir, Elephant Butte Reservoir, Flaming Gorge Reservoir, Navajo Reservoir, Great Salt Lake, Lake Mead, Lake Powell, Lake Mohave, Lake Tahoe, Pyramid Lake, & Salton Sea\n",
    "# # April 2017 – April 2020 @ Lake Mead & Lake Mohave validation sites\n",
    "# # June 2021 – June 2024 @ Lake Powell validation sites\n",
    "# # June 2021 – June 2024 @ Lake Tahoe validation sites\n",
    "\n",
    "\n",
    "# # Process the overpass LST July 2024 images for all reservoirs\n",
    "# for lake_name, lake_geometry in lake_geometries.items():\n",
    "#     print(f'\\n{lake_name}')\n",
    "#     asset_ingest(\n",
    "#         collection=\"pydms_sharpened_landsat\", \n",
    "#         extent=lake_geometry.bounds, \n",
    "#         start_date=\"2024-07-01T00:00:00Z\", \n",
    "#         end_date=\"2024-08-01T00:00:00Z\", \n",
    "#         overwrite_flag=True,\n",
    "#     )\n",
    "\n",
    "\n",
    "# # Process the interpolated daily LST July 2024 images for all reservoirs\n",
    "# for lake_name, lake_geometry in lake_geometries.items():\n",
    "#     print(f'\\n{lake_name}')\n",
    "#     asset_ingest(\n",
    "#         collection=\"starfm_predictions_modis_landsat\", \n",
    "#         extent=lake_geometry.bounds, \n",
    "#         start_date=\"2024-07-01T00:00:00Z\", \n",
    "#         end_date=\"2024-08-01T00:00:00Z\", \n",
    "#         overwrite_flag=False,\n",
    "#     )\n",
    "\n",
    "\n",
    "# # Process the overpass LST for the full timeseries for the main reservoirs\n",
    "# for year in [2024, 2023, 2022, 2021]:\n",
    "#     print('\\nTahoe')\n",
    "#     asset_ingest(\n",
    "#         collection=\"pydms_sharpened_landsat\", \n",
    "#         extent=lake_geometries['tahoe'].bounds, \n",
    "#         start_date=f\"{year}-01-01T00:00:00Z\", \n",
    "#         end_date=f\"{year+1}-01-01T00:00:00Z\", \n",
    "#         overwrite_flag=False,\n",
    "#     )\n",
    "\n",
    "# for year in [2024, 2023, 2022, 2021, 2020, 2019]:\n",
    "#     print('\\nPowell')\n",
    "#     asset_ingest(\n",
    "#         collection=\"pydms_sharpened_landsat\", \n",
    "#         extent=lake_geometries['powell'].bounds, \n",
    "#         start_date=f\"{year}-01-01T00:00:00Z\", \n",
    "#         end_date=f\"{year+1}-01-01T00:00:00Z\", \n",
    "#         overwrite_flag=False,\n",
    "#     )\n",
    "# for year in [2020, 2019, 2018, 2017]:\n",
    "#     print('\\nMead & Mohave')\n",
    "#     asset_ingest(\n",
    "#         collection=\"pydms_sharpened_landsat\", \n",
    "#         extent=lake_geometries['mead'].bounds, \n",
    "#         start_date=f\"{year}-01-01T00:00:00Z\", \n",
    "#         end_date=f\"{year+1}-01-01T00:00:00Z\", \n",
    "#         overwrite_flag=False,\n",
    "#     )\n",
    "\n",
    "# # Process the interpolated daily LST for the full timeseries for the main reservoirs\n",
    "# for year in [2024, 2023, 2022, 2021]:\n",
    "#     print('\\nTahoe')\n",
    "#     asset_ingest(\n",
    "#         collection=\"starfm_predictions_modis_landsat\", \n",
    "#         extent=lake_geometries['tahoe'].bounds, \n",
    "#         start_date=f\"{year}-01-01T00:00:00Z\", \n",
    "#         end_date=f\"{year+1}-01-01T00:00:00Z\", \n",
    "#         overwrite_flag=False,\n",
    "#     )\n",
    "\n",
    "# for year in [2024, 2023, 2022, 2021, 2020, 2019]:\n",
    "#     print('\\nPowell')\n",
    "#     asset_ingest(\n",
    "#         collection=\"starfm_predictions_modis_landsat\", \n",
    "#         extent=lake_geometries['powell'].bounds, \n",
    "#         start_date=f\"{year}-01-01T00:00:00Z\", \n",
    "#         end_date=f\"{year+1}-01-01T00:00:00Z\", \n",
    "#         overwrite_flag=False,\n",
    "#     )\n",
    "# for year in [2020, 2019, 2018, 2017]:\n",
    "#     print('\\nMead & Mohave')\n",
    "#     asset_ingest(\n",
    "#         collection=\"starfm_predictions_modis_landsat\", \n",
    "#         extent=lake_geometries['mead'].bounds, \n",
    "#         start_date=f\"{year}-01-01T00:00:00Z\", \n",
    "#         end_date=f\"{year+1}-01-01T00:00:00Z\", \n",
    "#         overwrite_flag=False,\n",
    "#     )\n",
    "\n",
    "# print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9579c3-c836-4732-8b3f-4938b7af791f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Then process the overpass LST for the full timeseries for the main reservoirs\n",
    "# for year in [2021, 2020, 2019]:\n",
    "#     print('\\nPowell')\n",
    "#     asset_ingest(\n",
    "#         collection=\"pydms_sharpened_landsat\", \n",
    "#         extent=lake_geometries['powell'].bounds, \n",
    "#         start_date=f\"{year}-01-01T00:00:00Z\", \n",
    "#         end_date=f\"{year+1}-01-01T00:00:00Z\", \n",
    "#         overwrite_flag=False,\n",
    "#     )\n",
    "\n",
    "# print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3299b75b-1245-484f-b198-e2dffae7de0c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Then process the interpolated daily LST for the full timeseries for the main reservoirs\n",
    "# for year in [2021, 2020, 2019]:\n",
    "#     print('\\nPowell')\n",
    "#     asset_ingest(\n",
    "#         collection=\"starfm_predictions_modis_landsat\", \n",
    "#         extent=lake_geometries['powell'].bounds, \n",
    "#         start_date=f\"{year}-01-01T00:00:00Z\", \n",
    "#         end_date=f\"{year+1}-01-01T00:00:00Z\", \n",
    "#         overwrite_flag=False,\n",
    "#     )\n",
    "\n",
    "# print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fdbd75-fca0-4fda-a7b5-fd994339b743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
